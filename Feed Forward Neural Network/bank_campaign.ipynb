{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bank_marketing_dataset = pd.read_csv(\"dataset/bank-full.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1506</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>unknown</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital  education default  balance housing loan  \\\n",
       "0   58    management  married   tertiary      no     2143     yes   no   \n",
       "1   44    technician   single  secondary      no       29     yes   no   \n",
       "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
       "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
       "4   33       unknown   single    unknown      no        1      no   no   \n",
       "\n",
       "   contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
       "0  unknown    5   may       261         1     -1         0  unknown  no  \n",
       "1  unknown    5   may       151         1     -1         0  unknown  no  \n",
       "2  unknown    5   may        76         1     -1         0  unknown  no  \n",
       "3  unknown    5   may        92         1     -1         0  unknown  no  \n",
       "4  unknown    5   may       198         1     -1         0  unknown  no  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_marketing_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = bank_marketing_dataset.iloc[:, [0, 1, 2, 3, 4, 5, 6, 7, 12, 13, 14, 15]]\n",
    "y = bank_marketing_dataset['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = pd.get_dummies(x, columns=['job', 'marital', 'education', 'default', 'housing', 'loan', \n",
    "                            'campaign', 'pdays', 'previous', 'poutcome'], drop_first=True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode the output variable into 1's & 0's\n",
    "label_encoder_y = LabelEncoder()\n",
    "\n",
    "y = label_encoder_y.fit_transform(y)\n",
    "y = y.reshape(y.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/deep-learning/lib/python3.6/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "feature_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "x = feature_scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape X and Y into array of (n_x, m)\n",
    "X_train = X_train.T\n",
    "y_train = y_train.T\n",
    "\n",
    "X_test = X_test.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in X: 669\n",
      "Number of Outputs (Y): 1\n",
      "Training examples: 40689\n",
      "Test examples: 4522\n"
     ]
    }
   ],
   "source": [
    "print(\"Features in X: \" + str(X_train.shape[0]))\n",
    "print(\"Number of Outputs (Y): \" + str(y_train.shape[0]))\n",
    "print(\"Training examples: \" + str(X_train.shape[1]))\n",
    "print(\"Test examples: \" + str(X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, number of features\n",
    "    n_y -- scalar, number of output classes\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \"\"\"\n",
    "    X = tf.placeholder(shape=[n_x, None], dtype=tf.float32)\n",
    "    Y = tf.placeholder(shape=[n_y, None], dtype=tf.float32)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params(layer_dimensions):\n",
    "    \"\"\"\n",
    "    Creates a python dicitonary that contains the weights & bias for each of the hidden layers\n",
    "    \n",
    "    Arguments:\n",
    "    layer_dimensions -- an integer array consisting of values representing number of hidden units in each layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary consisting of weights and bias parameters for each hidden layer\n",
    "    \"\"\"\n",
    "    \n",
    "    number_of_layers = len(layer_dimensions)\n",
    "    parameters = {}\n",
    "    \n",
    "    for layer in range(1, number_of_layers):\n",
    "        param_key_weight = \"W\" + str(layer)\n",
    "        param_key_bias = \"b\" + str(layer)\n",
    "        \n",
    "        parameters[param_key_weight] = tf.get_variable(name=param_key_weight, \n",
    "                                                       shape=[layer_dimensions[layer], layer_dimensions[(layer - 1)]], \n",
    "                                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "        parameters[param_key_bias] = tf.get_variable(name=param_key_bias, \n",
    "                                                     shape=[layer_dimensions[layer], 1], \n",
    "                                                     initializer=tf.zeros_initializer())\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, dropout=False):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (number of features, number of examples)\n",
    "    parameters -- python dictionary containing weights and bias parameters\n",
    "    dropout -- specifies whether to implement dropout or not\n",
    "\n",
    "    Returns:\n",
    "    Z_output -- the output of the last LINEAR unit \n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # the number of layers not including the input\n",
    "    A = X\n",
    "    \n",
    "    for layer in range(1, (L)):\n",
    "        Z = tf.matmul(parameters[\"W\" + str(layer)], A) + tf.cast(parameters[\"b\" + str(layer)], dtype=tf.float32)\n",
    "        A = tf.nn.relu(Z)\n",
    "        if dropout == True:\n",
    "            A = tf.nn.dropout(A, keep_prob=0.8)\n",
    "        \n",
    "    Z_output = tf.matmul(parameters[\"W\" + str(L)], A) + tf.cast(parameters[\"b\" + str(L)], dtype=tf.float32)\n",
    "    \n",
    "    return Z_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z_final, Y, parameters, loss_type='reglr', batch_size=1, lambd=0.001):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z_final -- output of forward propagation (output of the last LINEAR unit), of shape (number of features, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z_final\n",
    "    parameters -- python dictionary containing weights and bias parameters\n",
    "    loss_type -- type of loss to use\n",
    "    batch_size -- used to compute l2_loss\n",
    "    lambd -- lambda parameter which will be used for l2 loss\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    logits = tf.transpose(Z_final)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    if loss_type == 'reglr':\n",
    "        cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    elif loss_type == 'l2':\n",
    "        L = len(parameters) // 2 # the number of layers not including input\n",
    "        l2_loss = 0\n",
    "        \n",
    "        for layer in range(1, L):\n",
    "            l2_loss += tf.nn.l2_loss(parameters[\"W\" + str(layer)])\n",
    "            \n",
    "        l2_loss = (1 / batch_size) * lambd * l2_loss\n",
    "        sigmoid_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "        cost = l2_loss + sigmoid_loss\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, layer_dimensions, learning_rate=0.01, num_epochs=1500, \n",
    "          loss_type='reglr', dropout=False, print_cost=True):\n",
    "    \"\"\"\n",
    "    Implements a L-layer tensorflow neural network: (LINEAR->RELU) xL ->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (number of features, number of training examples)\n",
    "    Y_train -- test set, of shape (output size = 1, number of training examples)\n",
    "    X_test -- training set, of shape (number of features, number of training examples)\n",
    "    Y_test -- test set, of shape (output size = 1, number of test examples)\n",
    "    layer_dimensions -- dimensions, numer of units in each layer of the network\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    loss_type -- string, type of loss to use\n",
    "    dropout -- boolean, set True to implement dropout\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_params(layer_dimensions)\n",
    "    \n",
    "    # Forward propagation\n",
    "    Z_final = forward_propagation(X, parameters, dropout=dropout)\n",
    "    \n",
    "    # Compute cost\n",
    "    cost = compute_cost(Z_final, Y, parameters, loss_type=loss_type, batch_size=m)\n",
    "    \n",
    "    # Back Propagation Update weights \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Start session to compute tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            epoch_cost = 0      # Defines a cost related to an epoch\n",
    "            \n",
    "            _ , batch_cost = sess.run([optimizer, cost], feed_dict={X: X_train, Y:Y_train})\n",
    "            \n",
    "            epoch_cost += batch_cost\n",
    "            \n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "        \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z_final), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X_test, parameters):\n",
    "    \"\"\"\n",
    "    Returns predictions for a given set\n",
    "    Arguments:\n",
    "    X_test -- training set, of shape (number of features, number of training examples)\n",
    "    parameters -- python dictionary containing weights and bias parameters\n",
    "    \n",
    "    Returns:\n",
    "    y_pred -- numpy array that contains predictions for the X set\n",
    "    \"\"\"\n",
    "    (n_x, m) = X_test.shape\n",
    "    \n",
    "    X = tf.placeholder(shape=[n_x, None], dtype=tf.float32)\n",
    "    \n",
    "    y_pred = forward_propagation(X=X, parameters=parameters)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        y_pred = sess.run(y_pred, feed_dict={X: X_test})\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.690279\n",
      "Cost after epoch 100: 0.284748\n",
      "Cost after epoch 200: 0.238577\n"
     ]
    }
   ],
   "source": [
    "# Dimensions of the layer, change those numbers to increase nodes or \n",
    "#add another element between the 0th and the last element to add an extra layer\n",
    "layer_dimensions = [669, 450, 335, 1]\n",
    "\n",
    "parameters = model(X_train, y_train, X_test, y_test, layer_dimensions, \n",
    "                   learning_rate=0.001, num_epochs=1000, loss_type='reglr', dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = predict(X_test, parameters)\n",
    "y_pred = y_pred > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test.T, y_pred[0].T) #will need to transpose the matrix as it compare them with the 0th element\n",
    "print(\"Accuracy on test: %f\" % ((cm[0][0] + cm[1][1]) / float(y_test.shape[1]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
